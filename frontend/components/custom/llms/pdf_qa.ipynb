{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we build a PDF QA bot with open source models. We show how to use commercially available opensource models to query your knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Installations\n",
    "!pip install langchain==0.0.189\n",
    "!pip install chromadb==0.3.25\n",
    "!pip install pdfplumber==0.9.0\n",
    "!pip install tiktoken==0.4.0\n",
    "!pip install lxml==4.9.2\n",
    "!pip install torch==2.0.1\n",
    "!pip install transformers==4.29.2\n",
    "!pip install accelerate==0.19.0\n",
    "!pip install sentence-transformers==2.2.2\n",
    "!pip install einops==0.6.1\n",
    "!pip install xformers==0.0.20"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize OpenAI Keys \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import os\n",
    "try:\n",
    "    from hyperplane.utils import is_jhub\n",
    "    if is_jhub(): ##If using jupyter hub and conda base environment\n",
    "        openaiKeyFile = '/root/.secret/openai_key.json'\n",
    "    else:\n",
    "        ## Storing as a secret on k8s cluster\n",
    "        openaiKeyFile = '/etc/hyperplane/secrets/openai_key.json'\n",
    "    with open(openaiKeyFile) as f:\n",
    "        os.environ[\"OPENAI_API_KEY\"] = json.load(f)['openai_key']\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    load_dotenv()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step0: Loading LLM embedding models and generative models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Embedding Models\n",
    "* Opensource models used in the codebase are\n",
    "    * [INSTRUCTOR XL](https://huggingface.co/hkunlp/instructor-xl): Instructor xl is an instruction-finetuned text embedding model, that can generate embeddings tailored for any task instuction. The instruction for embedding text snippets is \"Represent the document for retrieval:\" and instruction for embedding user question is  \"Represent the question for retrieving supporting documents:\"\n",
    "    * [SBERT](https://huggingface.co/sentence-transformers/all-mpnet-base-v2): SBERT maps sentences and paragraphs to a vectore using BERT like model. It's a good starter when you're prototyping your application\n",
    "* Hugging faces [MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard) compares embedding models on  different tasks, Instructor XL ranks very high in it, even better than [OpenAI's ADA](https://platform.openai.com/docs/guides/embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_OPENAI_ADA = \"text-embedding-ada-002\"\n",
    "EMB_INSTRUCTOR_XL = \"hkunlp/instructor-xl\"\n",
    "EMB_SBERT_MPNET_BASE = \"sentence-transformers/all-mpnet-base-v2\" "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Generation Models\n",
    "* Opensource models used in the codebase are\n",
    "    * [FlanT5 Models](https://huggingface.co/docs/transformers/model_doc/flan-t5): FlanT5 is text2text generator that is finetuned on a number of tasks like summarisation, question answering. It uses the encode-decoder architecture of transformers. The model is Apache 2.0 licensed. It can used commercially.\n",
    "    * [FastChatT5 3b Model](https://huggingface.co/sentence-transformers/all-mpnet-base-v2): It's a FlanT5 based chat model trained by finetuning FlanT5 on user chats from ChatGpt. The model is Apache 2.0 licensed.\n",
    "    * [Falcon7b Model](https://huggingface.co/tiiuae/falcon-7b): Falcon7b is a smaller version of Falcon which is text generator model (decoder only model). Falcon-40B is currently the best open source model on the [OpenLLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard). They are trained with high quality training data\n",
    "\n",
    "There are other open-source models (MPT-7B, StableLM, RedPajama) in the [OpenLLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) you can consider for your tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "LLM_OPENAI_GPT35 = \"gpt-3.5-turbo\"\n",
    "LLM_FLAN_T5_XXL = \"google/flan-t5-xxl\"\n",
    "LLM_FLAN_T5_XL = \"google/flan-t5-xl\"\n",
    "LLM_FASTCHAT_T5_XL = \"lmsys/fastchat-t5-3b-v1.0\"\n",
    "LLM_FLAN_T5_SMALL = \"google/flan-t5-small\"\n",
    "LLM_FLAN_T5_BASE = \"google/flan-t5-base\"\n",
    "LLM_FLAN_T5_LARGE = \"google/flan-t5-large\"\n",
    "LLM_FALCON_SMALL = \"tiiuae/falcon-7b-instruct\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Falcon, FlanT5 LARGE,XL,XXL, Fastchat model requires GPU to run. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letâ€™s go ahead and first setup SBERT for embedding model and  flant5-base for generation model. Their inference latency is decent on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PDFPlumberLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter, TokenTextSplitter\n",
    "from transformers import pipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings, HuggingFaceEmbeddings\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"persist_directory\":None,\n",
    "          \"load_in_8bit\":False,\n",
    "          \"embedding\" : EMB_SBERT_MPNET_BASE,\n",
    "          \"llm\":LLM_FLAN_T5_BASE,\n",
    "          }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can be loaded and used for inference with the help of [hugging face pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines) which abstract away alot of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sbert_mpnet():\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        return HuggingFaceEmbeddings(model_name=EMB_SBERT_MPNET_BASE, model_kwargs={\"device\": device})\n",
    "\n",
    "\n",
    "def create_flan_t5_base(load_in_8bit=False):\n",
    "        # Wrap it in HF pipeline for use with LangChain\n",
    "        model=\"google/flan-t5-base\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        return pipeline(\n",
    "            task=\"text2text-generation\",\n",
    "            model=model,\n",
    "            tokenizer = tokenizer,\n",
    "            max_new_tokens=100,\n",
    "            model_kwargs={\"device_map\": \"auto\", \"load_in_8bit\": load_in_8bit, \"max_length\": 512, \"temperature\": 0.}\n",
    "        )\n",
    "\n",
    "def create_falcon_instruct_small(load_in_8bit=False):\n",
    "        model = \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        hf_pipeline = pipeline(\n",
    "                task=\"text-generation\",\n",
    "                model = model,\n",
    "                tokenizer = tokenizer,\n",
    "                trust_remote_code = True,\n",
    "                max_new_tokens=100,\n",
    "                model_kwargs={\n",
    "                    \"device_map\": \"auto\", \n",
    "                    \"load_in_8bit\": load_in_8bit, \n",
    "                    \"max_length\": 512, \n",
    "                    \"temperature\": 0.01,\n",
    "                    \"torch_dtype\":torch.bfloat16,\n",
    "                    }\n",
    "            )\n",
    "        return hf_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"embedding\"] == EMB_SBERT_MPNET_BASE:\n",
    "    embedding = create_sbert_mpnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_in_8bit = config[\"load_in_8bit\"]\n",
    "if config[\"llm\"] == LLM_FLAN_T5_BASE:\n",
    "    llm = create_flan_t5_base(load_in_8bit=load_in_8bit)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1: Ingesting the data into vector store (ChromaDB)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"wiki_data_short.pdf\"\n",
    "loader = PDFPlumberLoader(pdf_path)\n",
    "documents = loader.load()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split documents and create text snippets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "text_splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=10, encoding_name=\"cl100k_base\")  # This the encoding for text-embedding-ada-002\n",
    "texts = text_splitter.split_documents(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = config[\"persist_directory\"]\n",
    "vectordb = Chroma.from_documents(documents=texts, embedding=embedding, persist_directory=persist_directory)\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2: Retriving Snippets and Prompt Engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RetrievalQA finds relevant snippets based on question embeddings, then construct a Prompt and query LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_llm = HuggingFacePipeline(pipeline=llm)\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\":4})\n",
    "qa = RetrievalQA.from_chain_type(llm=hf_llm, chain_type=\"stuff\",retriever=retriever)\n",
    "# , chain_kwargs = {\"return_intermediate_steps\":True})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a default prompt for flan models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"llm\"] == LLM_FLAN_T5_SMALL or config[\"llm\"] == LLM_FLAN_T5_BASE or config[\"llm\"] == LLM_FLAN_T5_LARGE:\n",
    "    question_t5_template = \"\"\"\n",
    "    context: {context}\n",
    "    question: {question}\n",
    "    answer: \n",
    "    \"\"\"\n",
    "    QUESTION_T5_PROMPT = PromptTemplate(\n",
    "        template=question_t5_template, input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    qa.combine_documents_chain.llm_chain.prompt = QUESTION_T5_PROMPT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Querying LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what's the reason for financial crisis?\"\n",
    "qa.combine_documents_chain.verbose = True\n",
    "qa.return_source_documents = True\n",
    "qa({\"query\":question,})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pack all the functionalities into a class. We have also added Other opensource LLM Models into the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PdfQA:\n",
    "    def __init__(self,config:dict = {}):\n",
    "        self.config = config\n",
    "        self.embedding = None\n",
    "        self.vectordb = None\n",
    "        self.llm = None\n",
    "        self.qa = None\n",
    "        self.retriever = None\n",
    "\n",
    "    # The following class methods are useful to create global GPU model instances\n",
    "    # This way we don't need to reload models in an interactive app,\n",
    "    # and the same model instance can be used across multiple user sessions\n",
    "    @classmethod\n",
    "    def create_instructor_xl(cls):\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        return HuggingFaceInstructEmbeddings(model_name=EMB_INSTRUCTOR_XL, model_kwargs={\"device\": device})\n",
    "    \n",
    "    @classmethod\n",
    "    def create_sbert_mpnet(cls):\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        return HuggingFaceEmbeddings(model_name=EMB_SBERT_MPNET_BASE, model_kwargs={\"device\": device})\n",
    "    \n",
    "    @classmethod\n",
    "    def create_flan_t5_xxl(cls, load_in_8bit=False):\n",
    "        # Local flan-t5-xxl with 8-bit quantization for inference\n",
    "        # Wrap it in HF pipeline for use with LangChain\n",
    "        return pipeline(\n",
    "            task=\"text2text-generation\",\n",
    "            model=\"google/flan-t5-xxl\",\n",
    "            max_new_tokens=200,\n",
    "            model_kwargs={\"device_map\": \"auto\", \"load_in_8bit\": load_in_8bit, \"max_length\": 512, \"temperature\": 0.}\n",
    "        )\n",
    "    @classmethod\n",
    "    def create_flan_t5_xl(cls, load_in_8bit=False):\n",
    "        return pipeline(\n",
    "            task=\"text2text-generation\",\n",
    "            model=\"google/flan-t5-xl\",\n",
    "            max_new_tokens=200,\n",
    "            model_kwargs={\"device_map\": \"auto\", \"load_in_8bit\": load_in_8bit, \"max_length\": 512, \"temperature\": 0.}\n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def create_flan_t5_small(cls, load_in_8bit=False):\n",
    "        # Local flan-t5-small for inference\n",
    "        # Wrap it in HF pipeline for use with LangChain\n",
    "        model=\"google/flan-t5-small\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        return pipeline(\n",
    "            task=\"text2text-generation\",\n",
    "            model=model,\n",
    "            tokenizer = tokenizer,\n",
    "            max_new_tokens=100,\n",
    "            model_kwargs={\"device_map\": \"auto\", \"load_in_8bit\": load_in_8bit, \"max_length\": 512, \"temperature\": 0.}\n",
    "        )\n",
    "    @classmethod\n",
    "    def create_flan_t5_base(cls, load_in_8bit=False):\n",
    "        # Wrap it in HF pipeline for use with LangChain\n",
    "        model=\"google/flan-t5-base\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        return pipeline(\n",
    "            task=\"text2text-generation\",\n",
    "            model=model,\n",
    "            tokenizer = tokenizer,\n",
    "            max_new_tokens=100,\n",
    "            model_kwargs={\"device_map\": \"auto\", \"load_in_8bit\": load_in_8bit, \"max_length\": 512, \"temperature\": 0.}\n",
    "        )\n",
    "    @classmethod\n",
    "    def create_flan_t5_large(cls, load_in_8bit=False):\n",
    "        # Wrap it in HF pipeline for use with LangChain\n",
    "        model=\"google/flan-t5-large\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        return pipeline(\n",
    "            task=\"text2text-generation\",\n",
    "            model=model,\n",
    "            tokenizer = tokenizer,\n",
    "            max_new_tokens=100,\n",
    "            model_kwargs={\"device_map\": \"auto\", \"load_in_8bit\": load_in_8bit, \"max_length\": 512, \"temperature\": 0.}\n",
    "        )\n",
    "    @classmethod\n",
    "    def create_fastchat_t5_xl(cls, load_in_8bit=False):\n",
    "        return pipeline(\n",
    "            task=\"text2text-generation\",\n",
    "            model = \"lmsys/fastchat-t5-3b-v1.0\",\n",
    "            max_new_tokens=100,\n",
    "            model_kwargs={\"device_map\": \"auto\", \"load_in_8bit\": load_in_8bit, \"max_length\": 512, \"temperature\": 0.}\n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def create_falcon_instruct_small(cls, load_in_8bit=False):\n",
    "        model = \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        hf_pipeline = pipeline(\n",
    "                task=\"text-generation\",\n",
    "                model = model,\n",
    "                tokenizer = tokenizer,\n",
    "                trust_remote_code = True,\n",
    "                max_new_tokens=100,\n",
    "                model_kwargs={\n",
    "                    \"device_map\": \"auto\", \n",
    "                    \"load_in_8bit\": load_in_8bit, \n",
    "                    \"max_length\": 512, \n",
    "                    \"temperature\": 0.01,\n",
    "                    \"torch_dtype\":torch.bfloat16,\n",
    "                    }\n",
    "            )\n",
    "        return hf_pipeline\n",
    "    \n",
    "    def init_embeddings(self) -> None:\n",
    "        # OpenAI ada embeddings API\n",
    "        if self.config[\"embedding\"] == EMB_OPENAI_ADA:\n",
    "            self.embedding = OpenAIEmbeddings()\n",
    "        elif self.config[\"embedding\"] == EMB_INSTRUCTOR_XL:\n",
    "            # Local INSTRUCTOR-XL embeddings\n",
    "            if self.embedding is None:\n",
    "                self.embedding = PdfQA.create_instructor_xl()\n",
    "        elif self.config[\"embedding\"] == EMB_SBERT_MPNET_BASE:\n",
    "            ## this is for SBERT\n",
    "            if self.embedding is None:\n",
    "                self.embedding = PdfQA.create_sbert_mpnet()\n",
    "        else:\n",
    "            self.embedding = None ## DuckDb uses sbert embeddings\n",
    "            # raise ValueError(\"Invalid config\")\n",
    "\n",
    "    def init_models(self) -> None:\n",
    "        \"\"\" Initialize LLM models based on config \"\"\"\n",
    "        load_in_8bit = self.config.get(\"load_in_8bit\",False)\n",
    "        # OpenAI GPT 3.5 API\n",
    "        if self.config[\"llm\"] == LLM_OPENAI_GPT35:\n",
    "            # OpenAI GPT 3.5 API\n",
    "            pass\n",
    "        elif self.config[\"llm\"] == LLM_FLAN_T5_SMALL:\n",
    "            if self.llm is None:\n",
    "                self.llm = PdfQA.create_flan_t5_small(load_in_8bit=load_in_8bit)\n",
    "        elif self.config[\"llm\"] == LLM_FLAN_T5_BASE:\n",
    "            if self.llm is None:\n",
    "                self.llm = PdfQA.create_flan_t5_base(load_in_8bit=load_in_8bit)\n",
    "        elif self.config[\"llm\"] == LLM_FLAN_T5_LARGE:\n",
    "            if self.llm is None:\n",
    "                self.llm = PdfQA.create_flan_t5_large(load_in_8bit=load_in_8bit)\n",
    "        elif self.config[\"llm\"] == LLM_FLAN_T5_XL:\n",
    "            if self.llm is None:\n",
    "                self.llm = PdfQA.create_flan_t5_xl(load_in_8bit=load_in_8bit)\n",
    "        elif self.config[\"llm\"] == LLM_FLAN_T5_XXL:\n",
    "            if self.llm is None:\n",
    "                self.llm = PdfQA.create_flan_t5_xxl(load_in_8bit=load_in_8bit)\n",
    "        elif self.config[\"llm\"] == LLM_FASTCHAT_T5_XL:\n",
    "            if self.llm is None:\n",
    "                self.llm = PdfQA.create_fastchat_t5_xl(load_in_8bit=load_in_8bit)\n",
    "        elif self.config[\"llm\"] == LLM_FALCON_SMALL:\n",
    "            if self.llm is None:\n",
    "                self.llm = PdfQA.create_falcon_instruct_small(load_in_8bit=load_in_8bit)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Invalid config\")        \n",
    "    def vector_db_pdf(self) -> None:\n",
    "        \"\"\"\n",
    "        creates vector db for the embeddings and persists them or loads a vector db from the persist directory\n",
    "        \"\"\"\n",
    "        pdf_path = self.config.get(\"pdf_path\",None)\n",
    "        persist_directory = self.config.get(\"persist_directory\",None)\n",
    "        if persist_directory and os.path.exists(persist_directory):\n",
    "            ## Load from the persist db\n",
    "            self.vectordb = Chroma(persist_directory=persist_directory, embedding_function=self.embedding)\n",
    "        elif pdf_path and os.path.exists(pdf_path):\n",
    "            ## 1. Extract the documents\n",
    "            loader = PDFPlumberLoader(pdf_path)\n",
    "            documents = loader.load()\n",
    "            ## 2. Split the texts\n",
    "            text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "            texts = text_splitter.split_documents(documents)\n",
    "            # text_splitter = TokenTextSplitter(chunk_size=100, chunk_overlap=10, encoding_name=\"cl100k_base\")  # This the encoding for text-embedding-ada-002\n",
    "            text_splitter = TokenTextSplitter(chunk_size=100, chunk_overlap=10)  # This the encoding for text-embedding-ada-002\n",
    "            texts = text_splitter.split_documents(texts)\n",
    "\n",
    "            ## 3. Create Embeddings and add to chroma store\n",
    "            ##TODO: Validate if self.embedding is not None\n",
    "            self.vectordb = Chroma.from_documents(documents=texts, embedding=self.embedding, persist_directory=persist_directory)\n",
    "        else:\n",
    "            raise ValueError(\"NO PDF found\")\n",
    "\n",
    "    def retreival_qa_chain(self):\n",
    "        \"\"\"\n",
    "        Creates retrieval qa chain using vectordb as retrivar and LLM to complete the prompt\n",
    "        \"\"\"\n",
    "        ##TODO: Use custom prompt\n",
    "        self.retriever = self.vectordb.as_retriever(search_kwargs={\"k\":3})\n",
    "        \n",
    "        if self.config[\"llm\"] == LLM_OPENAI_GPT35:\n",
    "          # Use ChatGPT API\n",
    "          self.qa = RetrievalQA.from_chain_type(llm=OpenAI(model_name=LLM_OPENAI_GPT35, temperature=0.), chain_type=\"stuff\",\\\n",
    "                                      retriever=self.vectordb.as_retriever(search_kwargs={\"k\":3}))\n",
    "        else:\n",
    "            hf_llm = HuggingFacePipeline(pipeline=self.llm,model_id=self.config[\"llm\"])\n",
    "\n",
    "            self.qa = RetrievalQA.from_chain_type(llm=hf_llm, chain_type=\"stuff\",retriever=self.retriever)\n",
    "            if self.config[\"llm\"] == LLM_FLAN_T5_SMALL or self.config[\"llm\"] == LLM_FLAN_T5_BASE or self.config[\"llm\"] == LLM_FLAN_T5_LARGE:\n",
    "                question_t5_template = \"\"\"\n",
    "                context: {context}\n",
    "                question: {question}\n",
    "                answer: \n",
    "                \"\"\"\n",
    "                QUESTION_T5_PROMPT = PromptTemplate(\n",
    "                    template=question_t5_template, input_variables=[\"context\", \"question\"]\n",
    "                )\n",
    "                self.qa.combine_documents_chain.llm_chain.prompt = QUESTION_T5_PROMPT\n",
    "            self.qa.combine_documents_chain.verbose = True\n",
    "            self.qa.return_source_documents = True\n",
    "    def answer_query(self,question:str) ->str:\n",
    "        \"\"\"\n",
    "        Answer the question\n",
    "        \"\"\"\n",
    "\n",
    "        answer_dict = self.qa({\"query\":question,})\n",
    "        print(answer_dict)\n",
    "        answer = answer_dict[\"result\"]\n",
    "        if self.config[\"llm\"] == LLM_FASTCHAT_T5_XL:\n",
    "            answer = self._clean_fastchat_t5_output(answer)\n",
    "        return answer\n",
    "    def _clean_fastchat_t5_output(self, answer: str) -> str:\n",
    "        # Remove <pad> tags, double spaces, trailing newline\n",
    "        answer = re.sub(r\"<pad>\\s+\", \"\", answer)\n",
    "        answer = re.sub(r\"  \", \" \", answer)\n",
    "        answer = re.sub(r\"\\n$\", \"\", answer)\n",
    "        return answer\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the PDFQA class is set up, you can initialize and run it as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for PDFQA\n",
    "config = {\"persist_directory\":None,\n",
    "          \"load_in_8bit\":False,\n",
    "          \"embedding\" : EMB_SBERT_MPNET_BASE,\n",
    "          \"llm\":LLM_FLAN_T5_BASE,\n",
    "          \"pdf_path\":\"wiki_data_short.pdf\"\n",
    "          }\n",
    "\n",
    "# Initialize PDFQA\n",
    "pdfqa = PdfQA(config=config)\n",
    "pdfqa.init_embeddings()\n",
    "pdfqa.init_models()\n",
    "\n",
    "# Create Vector DB \n",
    "pdfqa.vector_db_pdf()\n",
    "\n",
    "# Set up Retrieval QA Chain\n",
    "pdfqa.retreival_qa_chain()\n",
    "\n",
    "# Query the model\n",
    "question = \"what the reason for financial crisis?\"\n",
    "pdfqa.answer_query(question)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
